import streamlit as st
import numpy as np 
import pandas as pd
import tensorflow as tf
import logging
import pandas as pd
from tensorflow.keras.layers import (
    Dense,
    Flatten,
    Conv1D,
    Dropout,
    Input,
)
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import Model
from transformers import TFBertModel
from transformers import BertTokenizer
model_name = "aubmindlab/bert-base-arabertv2"
max_length = 512
batch_size = 128
num_class = 95

tf.compat.v1.logging.set_verbosity(2)
tokenizer = BertTokenizer.from_pretrained(model_name)


try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.TPUStrategy(tpu)
except ValueError:
    strategy = tf.distribute.get_strategy() # for CPU and single GPU
    print('Number of replicas:', strategy.num_replicas_in_sync)

def arabert_encode(data):
    tokens = tokenizer.batch_encode_plus(
        data, max_length=max_length, padding="max_length", truncation=True
    )
    return tf.constant(tokens["input_ids"])

def arabert_model():
    bert_encoder = TFBertModel.from_pretrained(model_name, output_attentions=True)
    input_word_ids = Input(
        shape=(max_length,), dtype=tf.int32, name="input_ids"
    )
    last_hidden_states = bert_encoder(input_word_ids)[0]
    clf_output = Flatten()(last_hidden_states)
    net = Dense(512, activation="relu")(clf_output)
    net = Dropout(0.3)(net)
    net = Dense(440, activation="relu")(net)
    net = Dropout(0.3)(net)
    output = Dense(num_class, activation="softplus")(net)
    model = Model(inputs=input_word_ids, outputs=output)
    return model

from keras import backend as K

steps_per_exe =32

with strategy.scope():
  model = arabert_model()
  adam_optimizer = Adam(learning_rate=1e-5)
  model.compile(
        loss="categorical_crossentropy",
        optimizer=adam_optimizer,
        metrics=['acc'],
        steps_per_execution=steps_per_exe
    )

@st.cache(allow_output_mutation=True)
#model.load_weights("gs://axial-trail-334408-tf2-models/book-mnist")


#text = 'Ø§Ù„Ø±Ø£ÙŠ ÙØ¥Ù†Ù‡ Ù…ØªÙ‰ Ù…Ø§ Ø§ØªØ¨Ø¹ Ø§Ù„Ø±Ø£ÙŠ Ø¬Ø§Ø¡Ù‡ Ø±Ø¬Ù„ Ø¢Ø®Ø± Ø£Ù‚ÙˆÙ‰ ÙÙŠ Ø§Ù„Ø±Ø£ÙŠ Ù…Ù†Ù‡ ÙØ§ØªØ¨Ø¹Ù‡ ÙÙƒÙ„Ù…Ø§ ØºÙ„Ø¨Ù‡ Ø±Ø¬Ù„ Ø§ØªØ¨Ø¹Ù‡ Ø£Ø±Ù‰ Ø£Ù† Ù‡Ø°Ø§ Ø¨Ø¹Ø¯ Ù„Ù… ÙŠØªÙ… ÙˆØ§Ø¹Ù…Ù„ÙˆØ§ Ù…Ù† Ø§Ù„Ø¢Ø«Ø§Ø± Ø¨Ù…Ø§ Ø±ÙˆÙŠ Ø¹Ù† Ø¬Ø§Ø¨Ø± Ø±Ø¶ÙŠ Ø§Ù„Ù„Ù‡ Ø¹Ù†Ù‡ Ø£Ù† Ø§Ù„Ù†Ø¨ÙŠ ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù… Ù‚Ø§Ù„ Ù‚Ø¯ ØªØ±ÙƒØª ÙÙŠÙƒÙ… Ù…Ø§ Ù„Ù† ØªØ¶Ù„ÙˆØ§ Ø¨Ø¹Ø¯ÙŠ Ø¥Ø°Ø§ Ø§Ø¹ØªØµÙ…ØªÙ… Ø¨Ù‡ ÙƒØªØ§Ø¨ Ø§Ù„Ù„Ù‡ ÙˆØ³Ù†ØªÙŠ ÙˆÙ„Ù† ÙŠØªÙØ±Ù‚Ø§ Ø­ØªÙ‰ ÙŠØ±Ø¯Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ø­ÙˆØ¶ ÙˆØ±ÙˆÙŠ Ø¹Ù† Ø¹Ù…Ø±Ùˆ Ø¨Ù† Ø´Ø¹ÙŠØ¨ Ø¹Ù† Ø£Ø¨ÙŠÙ‡ Ø¹Ù† Ø¬Ø¯Ù‡ Ø®Ø±Ø¬ Ø±Ø³ÙˆÙ„ Ø§Ù„Ù„Ù‡ ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù… ÙŠÙˆÙ…Ø§ ÙˆÙ‡Ù… ÙŠØ¬Ø§Ø¯Ù„ÙˆÙ† ÙÙŠ Ø§Ù„Ù‚Ø±Ø¢Ù† ÙØ®Ø±Ø¬ ÙˆÙˆØ¬Ù‡Ù‡ Ø£Ø­Ù…Ø± ÙƒØ§Ù„Ø¯Ù… ÙÙ‚Ø§Ù„ ÙŠØ§ Ù‚ÙˆÙ… Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ù‡Ù„Ùƒ Ù…Ù† ÙƒØ§Ù† Ù‚Ø¨Ù„ÙƒÙ… Ø¬Ø§Ø¯Ù„ÙˆØ§ ÙÙŠ Ø§Ù„Ù‚Ø±Ø¢Ù† ÙˆØ¶Ø±Ø¨ÙˆØ§ Ø¨Ø¹Ø¶Ù‡ Ø¨Ø¨Ø¹Ø¶ ÙÙ…Ø§ ÙƒØ§Ù† Ù…Ù† Ø­Ù„Ø§Ù„ ÙØ§Ø¹Ù…Ù„ÙˆØ§ Ø¨Ù‡ ÙˆÙ…Ø§ ÙƒØ§Ù† Ù…Ù† Ø­Ø±Ø§Ù… ÙØ§Ù†ØªÙ‡ÙˆØ§ Ø¹Ù†Ù‡ ÙˆÙ…Ø§ ÙƒØ§Ù† Ù…Ù† Ù…ØªØ´Ø§Ø¨Ù‡ ÙØ¢Ù…Ù†ÙˆØ§ Ø¨Ù‡'
text_encoded = arabert_encode(text)
p = (
    tf.data.Dataset.from_tensor_slices((text_encoded))
    .batch(batch_size)
).cache()
y_pred = model.predict(p, verbose=2)
	
	
author , book = df.iloc[[np.argmax(x) for x in y_pred][0]].tolist()
st.write('Author: ',author , '\nBook: ', book, '\nConfidence:', y_pred[0][[np.argmax(x) for x in y_pred][0]])
	
	
	
	
	
	
	
#pio.renderers.default = 'chrome'
st.set_option('deprecation.showPyplotGlobalUse', False)
#st.set_page_config(layout="wide")

#st.title('Recommended for you!')
st.markdown(' <p align="center" class="big-font">  <b>Authorship Attribution <u> ğŸŒŸ T5 ğŸ‡¸ğŸ‡¦</b>   </p>', unsafe_allow_html=True)	


st.markdown("""
<style>
.big-font {
    font-size:50px !important;
}
</style>
""", unsafe_allow_html=True)



st.markdown("""
Ø¥Ø³Ù†Ø§Ø¯ Ø§Ù„ØªØ£Ù„ÙŠÙ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù‡Ùˆ Ù…Ù‡Ù…Ø© Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø¤Ù„Ù Ø§Ù„Ù…Ø³ØªÙ†Ø¯. Ù„ØªØ­Ù‚ÙŠÙ‚ Ù‡Ø°Ø§ Ø§Ù„ØºØ±Ø¶ ØŒ ÙŠÙ‚Ø§Ø±Ù† Ø§Ù„Ù…Ø±Ø¡ Ù†Øµ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¤Ù„Ù Ø§Ù„Ù…Ø±Ø´Ø­ ÙˆÙŠØ­Ø¯Ø¯ Ø§Ø­ØªÙ…Ø§Ù„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù….

Arabic authorship attribution is the task of finding the author of a document.
To achieve this purpose, one compares a query text with a model of the candidate author and determines the likelihood of the model for the query.
	""")

#text = st.text_area("Authorship Attribution Check","Enter Text Here")










st.write('---')
st.write('## Contact Our Group')


st.write("""
[Authorship Attribution](https://github.com/A-safarji) - feel free to contact!
""")
                               
